# Capstone #5
- Update on using UniMelb Research Cloud
- Re-iterating our Project Timeline (Adjusting Timeline)
- Re-assigning tasks:
  - Team Assignment 01;
  - Figure out how to develop gesture-control ontop of openpose:
    a. Through api plugins;
    b. Modifying openpose framework?
    
    
## Minutes

### Recap from last week
- Apart from HCI, we can look into Action Estimation
- Pose estimation fixed on OpenPose
- Example on PoseNet: example by Maya Man (Link to be shared by Yick)
- Pose estimation algorithm rubric (could be useful for reports)
- Work on block diagram of our system (main task)

## Concerns
- resource: http://blog.leapmotion.com/sculpting-sound-real-time-supercollider/
- Whether it is efficient to do pose estimation and gesture recognition separately
- Whether RGB camera is sufficient

### Unimelb Research Cloud
(@Matthew can show a quick demo)

## Links
- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4507703/
- https://www.vs.inf.ethz.ch/edu/FS2014/UCS/reports/AdrianSpurr_HandPoseEstimation_report.

## To-do (Moving Forward)
* Setting up Cloud Research Instance - Matthew 
* Set up template for Assignment 01 - Yick
* Setting up Latex (overleaf) - Yick
* Play around with OpenPose on Google Colab - Team
* Gesture Research - Tsz Kiu
* Go through Lecture Materials for Assignment 01 - Team
* Jonathan Update Email - Tsz Kiu (get in done by 03/04)
* Three options:
    * Web application (using Research Cloud)
    * Google colab
    * Use research cloud as part of our computing, and the rest on our local machine